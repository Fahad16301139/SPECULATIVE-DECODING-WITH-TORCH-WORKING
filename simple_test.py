#!/usr/bin/env python3
"""
Simple test for improved speculative decoding context management.
Uses the existing working infrastructure.
"""

import os
import requests
import time

# Set debug mode
os.environ["DEBUG"] = "1"

def test_context_management():
    print("ğŸ”§ ========== SIMPLE CONTEXT MANAGEMENT TEST ==========")
    print("ğŸ¯ Testing cache coordination improvements")
    print()
    
    # Test API endpoint (assumes server is running)
    url = "http://localhost:52415/v1/chat/completions"
    
    headers = {
        "Content-Type": "application/json"
    }
    
    # Test with a context-dependent prompt
    test_data = {
        "model": "meta-llama/Llama-3.2-3B",
        "messages": [
            {
                "role": "user", 
                "content": "Count from 1 to 10: One, Two, Three"
            }
        ],
        "max_tokens": 20,
        "temperature": 0.8,
        "stream": False
    }
    
    print("ğŸ“ Test prompt: 'Count from 1 to 10: One, Two, Three'")
    print("ğŸ¯ Expected: Should continue with 'Four, Five, Six...'")
    print()
    
    try:
        print("ğŸš€ Sending request...")
        start_time = time.time()
        
        response = requests.post(url, json=test_data, headers=headers, timeout=60)
        
        end_time = time.time()
        
        if response.status_code == 200:
            result = response.json()
            content = result['choices'][0]['message']['content']
            
            print("âœ… SUCCESS!")
            print(f"â±ï¸  Response time: {(end_time - start_time):.2f}s")
            print(f"ğŸ“„ Response: '{content}'")
            print()
            
            # Check context continuity
            if any(word in content.lower() for word in ['four', '4', 'five', '5']):
                print("âœ… CONTEXT CONTINUITY: GOOD - Sequence continues properly!")
                print("ğŸ”§ Cache coordination is working correctly")
            else:
                print("âš ï¸  CONTEXT CONTINUITY: Check needed - Response may not follow context")
            
            # Check for any debug output patterns that indicate cache issues
            if "cache_pos" in content or "overflow" in content.lower():
                print("âš ï¸  Possible cache issues detected in output")
            else:
                print("âœ… No cache error patterns detected")
                
        else:
            print(f"âŒ Request failed with status {response.status_code}")
            print(f"Response: {response.text}")
            
    except requests.exceptions.ConnectionError:
        print("âŒ CONNECTION ERROR: Server not running!")
        print("ğŸ’¡ Start the server first with:")
        print("   python exo/main.py --inference-engine SpeculativeInferenceEngine --model meta-llama/Llama-3.2-3B --draft-model meta-llama/Llama-3.2-1B --port 52415")
        return False
        
    except Exception as e:
        print(f"âŒ TEST FAILED: {e}")
        return False
    
    print()
    print("ğŸ”§ ========== TEST COMPLETE ==========")
    return True

if __name__ == "__main__":
    test_context_management() 