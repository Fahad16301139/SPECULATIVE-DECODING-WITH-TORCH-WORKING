#!/usr/bin/env python3
"""
Test script for improved context management in speculative decoding.
Tests the cache coordination and position management fixes.
"""

import os
import asyncio
import numpy as np
import time

# Set debug mode
os.environ["DEBUG"] = "2"

from exo.inference.torch.sharded_inference_engine import TorchDynamicShardInferenceEngine
from exo.inference.speculative_inference_engine import SpeculativeInferenceEngine
from exo.inference.shard import Shard

async def test_context_management():
    print("ğŸ”§ ========== TESTING IMPROVED CONTEXT MANAGEMENT ==========")
    print("ğŸ¯ Focus: Cache coordination and position management")
    print()
    
    # Create target and draft engines
    print("ğŸ“¦ Setting up engines...")
    from exo.download.shard_download import ShardDownloader
    
    target_engine = TorchDynamicShardInferenceEngine(ShardDownloader())
    draft_engine = TorchDynamicShardInferenceEngine(ShardDownloader())
    
    # Create speculative engine with cache coordination
    spec_engine = SpeculativeInferenceEngine(
        target_engine=target_engine,
        draft_engine=draft_engine,
        gamma=4,  # Smaller gamma for clearer testing
        temperature=0.8,
        target_model_id="meta-llama/Llama-3.2-3B",
        draft_model_id="meta-llama/Llama-3.2-1B"
    )
    
    # Create test shards
    target_shard = Shard(
        model_id="meta-llama/Llama-3.2-3B",
        start_layer=0,
        end_layer=28,
        n_layers=28
    )
    
    print("âœ… Engines created successfully")
    print()
    
    # Test 1: Context building with cache coordination
    print("ğŸ§ª TEST 1: Context Building with Cache Coordination")
    print("=" * 50)
    
    # Start with a simple prompt that requires context
    test_prompt = "Count from 1 to 10: One, Two, Three"
    
    try:
        # Encode prompt
        print(f"ğŸ“ Encoding prompt: '{test_prompt}'")
        input_tokens = await spec_engine.encode(target_shard, test_prompt)
        print(f"ğŸ”¢ Input tokens shape: {input_tokens.shape}")
        print(f"ğŸ”¢ Input tokens (last 10): {input_tokens[0, -10:].tolist()}")
        print()
        
        # Test inference with cache coordination
        print("ğŸ”® Running speculative inference with cache coordination...")
        start_time = time.perf_counter()
        
        result_tokens, final_cache, accepted_tokens = await spec_engine.infer_tensor_multi(
            request_id="context_test_1",
            shard=target_shard,
            input_data=input_tokens,
            inference_state=None
        )
        
        end_time = time.perf_counter()
        
        print(f"â±ï¸  Inference time: {(end_time - start_time)*1000:.2f}ms")
        print(f"ğŸ¯ Result tokens shape: {result_tokens.shape}")
        print(f"ğŸ¯ Result tokens (last 10): {result_tokens[0, -10:].tolist()}")
        print(f"âœ… Accepted tokens: {accepted_tokens}")
        print()
        
        # Decode the result
        print("ğŸ“„ Decoding result...")
        decoded_result = await spec_engine.decode(target_shard, result_tokens)
        print(f"ğŸ“„ Decoded text: '{decoded_result}'")
        print()
        
        # Verify context continuity
        if "Four" in decoded_result or "4" in decoded_result:
            print("âœ… CONTEXT CONTINUITY: Good - sequence continues properly")
        else:
            print("âš ï¸  CONTEXT CONTINUITY: May have issues - check if sequence flows")
        
        print()
        
    except Exception as e:
        print(f"âŒ TEST 1 FAILED: {e}")
        import traceback
        traceback.print_exc()
        print()
    
    # Test 2: Cache position management under stress
    print("ğŸ§ª TEST 2: Cache Position Management Under Stress")
    print("=" * 50)
    
    try:
        # Create a longer context to stress test cache management
        long_context = "The quick brown fox jumps over the lazy dog. " * 5
        long_context += "Now let's continue the story with more details about the fox and the dog. "
        
        print(f"ğŸ“ Long context length: {len(long_context)} characters")
        
        # Encode long context
        long_input_tokens = await spec_engine.encode(target_shard, long_context)
        print(f"ğŸ”¢ Long input shape: {long_input_tokens.shape}")
        
        # Run multiple inference steps to test cache coordination
        current_tokens = long_input_tokens
        cache_state = None
        
        for step in range(3):
            print(f"\nğŸ”„ Cache stress test step {step + 1}/3")
            
            step_result, cache_state, step_accepted = await spec_engine.infer_tensor_multi(
                request_id=f"cache_stress_{step}",
                shard=target_shard,
                input_data=current_tokens,
                inference_state=cache_state
            )
            
            print(f"   Step {step + 1} tokens accepted: {len(step_accepted) if step_accepted else 0}")
            print(f"   Cache state type: {type(cache_state)}")
            if hasattr(cache_state, 'cache_pos'):
                print(f"   Cache position: {getattr(cache_state, 'cache_pos', 'N/A')}")
            
            # Update for next iteration
            current_tokens = step_result
            
        print("âœ… CACHE STRESS TEST: All steps completed successfully")
        
        # Final decode to check overall coherence
        final_decoded = await spec_engine.decode(target_shard, current_tokens)
        print(f"ğŸ“„ Final decoded length: {len(final_decoded)} characters")
        print(f"ğŸ“„ Final text preview: '{final_decoded[-100:]}'")
        
    except Exception as e:
        print(f"âŒ TEST 2 FAILED: {e}")
        import traceback
        traceback.print_exc()
    
    print()
    print("ğŸ”§ ========== CONTEXT MANAGEMENT TEST COMPLETE ==========")
    
    # Summary of improvements tested
    print("\nğŸ“Š IMPROVEMENTS TESTED:")
    print("âœ“ Cache position coordination between draft and target")
    print("âœ“ Cache state backup and restoration")
    print("âœ“ Fallback strategies for cache overflow")
    print("âœ“ Context continuity across multiple inference steps")
    print("âœ“ Proper sequence building with cache management")

if __name__ == "__main__":
    asyncio.run(test_context_management()) 